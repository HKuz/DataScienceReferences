{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Program Variables\n",
    "num_iters = 1500  # Number of iterations for gradient descent\n",
    "alpha = 0.01  # Gradient descent learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model Representation\n",
    "\n",
    "Given the following assumptions:\n",
    "- Training set contains $m$ observations with $n$ features\n",
    "- Training set of features $X = [(x_1^{(1)}, x_2^{(1)}, \\ldots, x_n^{(1)}), \\ldots , (x_1^{(m)}, x_2^{(m)}, \\ldots, x_n^{(m)}) ]$\n",
    "- Training set of labels $y = [y^{(1)}, \\ldots , y^{(m)}]$\n",
    "- Parameter vector $\\theta = [\\theta_0, \\ldots , \\theta_n]$\n",
    "\n",
    "Goal is to create a hypothesis function $h_{\\theta}(x)$ with parameters (coefficients) $\\theta$ in order to accurately calculate the label for a new, unseen $x$. The form of the hypothesis function is:\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = \\theta_0 + \\theta_1x_1 + \\ldots + \\theta_nx_n\n",
    "$$\n",
    "\n",
    "Assuming $x_0 = 1$ for all observations, this is equivalent to:\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = \\displaystyle \\sum_{j=0}^{n} \\theta_jx_j^{(i)}\n",
    "$$\n",
    "\n",
    "You can add a new first column to $X$ applying the assumption $x_0^{(i)} = 1$ for all observations. This creates a design matrix of dimension $m \\times (n+1)$ that's compatible to perform matrix multiplication with the $(n+1) \\times 1$ vector $\\theta$. The vectorized representation of the hypothesis function (that results in an $m \\times 1$ vector with $h_{\\theta}(x^{(i)})$ for all observations, $i$ to $m$) is:\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = X\\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Cost Function\n",
    "\n",
    "Given a training set of $m$ features $X$, $m$ labels $Y$, and $n$ parameters $\\theta$, want to minimize the following cost function:\n",
    "\n",
    "$$\n",
    "J(\\theta_0, \\ldots, \\theta_n) = J(\\theta) = \\frac{1}{2m} \\displaystyle \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "Vectorized formula:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} (X\\theta - y)^{T} (X\\theta - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cost(X, y, theta):\n",
    "    '''\n",
    "    Calculates the cost of using theta as the parameters for linear\n",
    "        regression to fit the data points in X and y\n",
    "    X: mx(n+1) design matrix of training set features\n",
    "    y: mx1 vector of training set labels\n",
    "    theta: (n+1)x1 vector of parameters\n",
    "    Output: J(theta), float\n",
    "    '''\n",
    "    m = len(y)  # Number of training observations\n",
    "    # J = (1/(2*m)) * ((X * theta - y)^T * (X * theta - y));\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Method\n",
    "\n",
    "Gradient descent formula:\n",
    "\n",
    "Repeat until convergence {  \n",
    "$\\quad \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta) $  \n",
    "}\n",
    "\n",
    "Substituting the partial derivative of the cost function:\n",
    "\n",
    "Repeat {  \n",
    "$ \\quad \\theta_j := \\theta_j - \\frac {\\alpha}{m} \\displaystyle \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} $  \n",
    "}\n",
    "\n",
    "Vectorized formula:\n",
    "\n",
    "$\\theta := \\theta - \\frac {\\alpha}{m} (X^T (X\\theta - y))$\n",
    "\n",
    "Note that $X$ is the design matrix of features, and should be scaled appropriately to help the gradient descent algorithm converge more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equations Method\n",
    "\n",
    "There's an analytical solution "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
